# tests/devops_master_test_orchestrator.py
"""
Comprehensive DevOps Testing & Architecture Validation
Master Test Orchestrator

Coordinates and executes all testing frameworks for PsychSync AI platform.
Provides unified reporting, executive dashboard, and CI/CD integration.
"""

import asyncio
import json
import logging
import time
import uuid
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timezone
import argparse

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

# Import all testing frameworks
from devops_architecture_validation import ArchitectureValidator, run_architecture_validation
from devops_functional_testing import FunctionalTestFramework, run_functional_testing
from devops_performance_testing import PerformanceTestFramework, run_performance_testing

logger = logging.getLogger(__name__)

@dataclass
class TestSuiteResult:
    """Individual test suite result"""
    suite_name: str
    status: str  # 'PASS', 'FAIL', 'ERROR', 'SKIPPED'
    execution_time: float
    details: str
    report_path: Optional[str] = None
    grade: Optional[str] = None
    score: float = 0.0
    critical_issues: List[str] = None

    def __post_init__(self):
        if self.critical_issues is None:
            self.critical_issues = []

@dataclass
class MasterTestReport:
    """Master test report containing all test suite results"""
    timestamp: str
    overall_status: str
    overall_grade: str
    overall_score: float
    execution_time: float
    test_suites: List[TestSuiteResult]
    summary: Dict[str, Any]
    recommendations: List[str]
    production_readiness: Dict[str, Any]
    executive_summary: str

class MasterTestOrchestrator:
    """Master test orchestrator for all testing frameworks"""

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.test_reports_dir = self.project_root / "test_reports"
        self.test_reports_dir.mkdir(exist_ok=True)

        self.test_suites = [
            {
                'name': 'Architecture Validation',
                'description': 'Layer separation, dependency injection, async consistency',
                'critical': True,
                'weight': 0.25,
                'function': self._run_architecture_tests
            },
            {
                'name': 'Functional Testing',
                'description': 'API endpoints, database operations, email service',
                'critical': True,
                'weight': 0.30,
                'function': self._run_functional_tests
            },
            {
                'name': 'Performance Testing',
                'description': 'Load testing, stress testing, endurance testing',
                'critical': True,
                'weight': 0.25,
                'function': self._run_performance_tests
            },
            {
                'name': 'Security Assessment',
                'description': 'Vulnerability scanning, security headers, authentication',
                'critical': True,
                'weight': 0.20,
                'function': self._run_security_tests
            }
        ]

    async def run_all_tests(self, skip_non_critical: bool = False,
                           parallel: bool = False,
                           verbose: bool = True) -> MasterTestReport:
        """Run all test suites"""
        start_time = time.time()
        test_suites_results = []

        logger.info("üöÄ Starting Master Test Orchestration...")
        print("üöÄ Master Test Orchestration Starting...")
        print("=" * 60)

        if verbose:
            print(f"üìã Test Suites to Run:")
            for suite in self.test_suites:
                critical_icon = "üî¥" if suite['critical'] else "üü°"
                print(f"   {critical_icon} {suite['name']} - {suite['description']}")

        if parallel:
            # Run tests in parallel
            test_suites_results = await self._run_tests_parallel(skip_non_critical, verbose)
        else:
            # Run tests sequentially
            test_suites_results = await self._run_tests_sequential(skip_non_critical, verbose)

        execution_time = time.time() - start_time

        # Generate master report
        master_report = await self._generate_master_report(
            test_suites_results, execution_time, verbose
        )

        # Save master report
        await self._save_master_report(master_report)

        # Print executive summary
        if verbose:
            self._print_executive_summary(master_report)

        return master_report

    async def _run_tests_sequential(self, skip_non_critical: bool,
                                  verbose: bool) -> List[TestSuiteResult]:
        """Run test suites sequentially"""
        results = []

        for suite in self.test_suites:
            if skip_non_critical and not suite['critical']:
                if verbose:
                    print(f"‚è≠Ô∏è  Skipping {suite['name']} (non-critical)")
                results.append(TestSuiteResult(
                    suite_name=suite['name'],
                    status='SKIPPED',
                    execution_time=0.0,
                    details='Skipped by user request'
                ))
                continue

            if verbose:
                print(f"\nüß™ Running {suite['name']}...")

            try:
                start_time = time.time()
                result = await suite['function']()
                execution_time = time.time() - start_time

                if isinstance(result, TestSuiteResult):
                    result.execution_time = execution_time
                    results.append(result)
                else:
                    # Convert to TestSuiteResult
                    results.append(TestSuiteResult(
                        suite_name=suite['name'],
                        status='PASS',
                        execution_time=execution_time,
                        details='Test completed successfully'
                    ))

            except Exception as e:
                logger.error(f"Test suite {suite['name']} failed: {str(e)}")
                results.append(TestSuiteResult(
                    suite_name=suite['name'],
                    status='ERROR',
                    execution_time=0.0,
                    details=f'Test suite failed: {str(e)}',
                    critical_issues=[f"Critical error in {suite['name']}: {str(e)}"]
                ))

        return results

    async def _run_tests_parallel(self, skip_non_critical: bool,
                                 verbose: bool) -> List[TestSuiteResult]:
        """Run test suites in parallel"""
        # Filter suites to run
        suites_to_run = [
            suite for suite in self.test_suites
            if not skip_non_critical or suite['critical']
        ]

        # Create tasks
        tasks = []
        for suite in suites_to_run:
            if verbose:
                print(f"üß™ Queueing {suite['name']}...")
            task = asyncio.create_task(
                self._run_suite_with_timing(suite),
                name=suite['name']
            )
            tasks.append(task)

        # Execute all tasks
        if verbose:
            print(f"\n‚ö° Running {len(tasks)} test suites in parallel...")

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results
        processed_results = []
        for i, result in enumerate(results):
            suite_name = suites_to_run[i]['name']

            if isinstance(result, Exception):
                processed_results.append(TestSuiteResult(
                    suite_name=suite_name,
                    status='ERROR',
                    execution_time=0.0,
                    details=f'Parallel execution failed: {str(result)}',
                    critical_issues=[f"Parallel execution error in {suite_name}: {str(result)}"]
                ))
            elif isinstance(result, TestSuiteResult):
                processed_results.append(result)
            else:
                processed_results.append(TestSuiteResult(
                    suite_name=suite_name,
                    status='PASS',
                    execution_time=0.0,
                    details='Test completed successfully'
                ))

        # Add skipped tests if any
        if skip_non_critical:
            for suite in self.test_suites:
                if not suite['critical'] and suite['name'] not in [r.suite_name for r in processed_results]:
                    processed_results.append(TestSuiteResult(
                        suite_name=suite['name'],
                        status='SKIPPED',
                        execution_time=0.0,
                        details='Skipped by user request'
                    ))

        return processed_results

    async def _run_suite_with_timing(self, suite: Dict[str, Any]) -> TestSuiteResult:
        """Run a test suite with timing"""
        start_time = time.time()
        try:
            result = await suite['function']()
            execution_time = time.time() - start_time

            if isinstance(result, TestSuiteResult):
                result.execution_time = execution_time
                return result
            else:
                return TestSuiteResult(
                    suite_name=suite['name'],
                    status='PASS',
                    execution_time=execution_time,
                    details='Test completed successfully'
                )
        except Exception as e:
            execution_time = time.time() - start_time
            return TestSuiteResult(
                suite_name=suite['name'],
                status='ERROR',
                execution_time=execution_time,
                details=f'Test suite failed: {str(e)}',
                critical_issues=[f"Error in {suite['name']}: {str(e)}"]
            )

    async def _run_architecture_tests(self) -> TestSuiteResult:
        """Run architecture validation tests"""
        print("   üèóÔ∏è  Analyzing application architecture...")

        # Run the architecture validation
        architecture_report = run_architecture_validation()

        # Extract key metrics
        summary = architecture_report.get('summary', {})
        architecture_grade = architecture_report.get('architecture_grade', 'NO_GRADE')

        # Determine status
        if architecture_grade in ['A', 'B+', 'B']:
            status = 'PASS'
            details = f"Architecture validation passed with grade {architecture_grade}"
        elif architecture_grade == 'C':
            status = 'FAIL'
            details = f"Architecture validation needs improvement (Grade: {architecture_grade})"
        else:
            status = 'FAIL'
            details = f"Critical architecture issues found (Grade: {architecture_grade})"

        # Identify critical issues
        critical_issues = []
        if summary.get('failed', 0) > 0:
            critical_issues.append(f"{summary['failed']} failed architecture tests")

        return TestSuiteResult(
            suite_name='Architecture Validation',
            status=status,
            execution_time=0.0,  # Will be set by orchestrator
            details=details,
            grade=architecture_grade,
            score=self._grade_to_score(architecture_grade),
            critical_issues=critical_issues
        )

    async def _run_functional_tests(self) -> TestSuiteResult:
        """Run functional testing"""
        print("   üß™ Testing application functionality...")

        # Run functional tests
        functional_report = await run_functional_testing()

        summary = functional_report.get('summary', {})
        functional_grade = functional_report.get('functional_grade', 'NO_GRADE')
        success_rate = summary.get('success_rate', 0)

        # Determine status
        if functional_grade in ['A', 'B+', 'B'] and success_rate >= 80:
            status = 'PASS'
            details = f"Functional tests passed with {success_rate:.1f}% success rate (Grade: {functional_grade})"
        elif success_rate >= 60:
            status = 'FAIL'
            details = f"Functional tests have issues: {success_rate:.1f}% success rate (Grade: {functional_grade})"
        else:
            status = 'FAIL'
            details = f"Critical functional issues: {success_rate:.1f}% success rate"

        # Critical issues
        critical_issues = []
        if summary.get('failed', 0) > 0:
            critical_issues.append(f"{summary['failed']} failed functional tests")

        # Save report
        report_path = self.test_reports_dir / f"functional_testing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        return TestSuiteResult(
            suite_name='Functional Testing',
            status=status,
            execution_time=0.0,
            details=details,
            report_path=str(report_path),
            grade=functional_grade,
            score=success_rate,
            critical_issues=critical_issues
        )

    async def _run_performance_tests(self) -> TestSuiteResult:
        """Run performance testing"""
        print("   ‚ö° Testing application performance...")

        # Run performance tests
        performance_report = await run_performance_testing()

        summary = performance_report.get('summary', {})
        performance_grade = performance_report.get('performance_grade', 'NO_GRADE')
        success_rate = summary.get('success_rate', 0)

        # Determine status
        if performance_grade in ['A', 'B+', 'B']:
            status = 'PASS'
            details = f"Performance tests passed with grade {performance_grade}"
        elif performance_grade == 'C':
            status = 'FAIL'
            details = f"Performance tests show degradation (Grade: {performance_grade})"
        else:
            status = 'FAIL'
            details = f"Critical performance issues found (Grade: {performance_grade})"

        # Critical issues
        critical_issues = []
        if summary.get('failed', 0) > 0:
            critical_issues.append(f"{summary['failed']} failed performance tests")

        # Save report
        report_path = self.test_reports_dir / f"performance_testing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        return TestSuiteResult(
            suite_name='Performance Testing',
            status=status,
            execution_time=0.0,
            details=details,
            report_path=str(report_path),
            grade=performance_grade,
            score=self._grade_to_score(performance_grade),
            critical_issues=critical_issues
        )

    async def _run_security_tests(self) -> TestSuiteResult:
        """Run security assessment (placeholder for now)"""
        print("   üîí Running security assessment...")

        # Placeholder for security tests
        # In a real implementation, this would run vulnerability scanners,
        # check security headers, test authentication, etc.

        # Simulate security test execution
        await asyncio.sleep(2)

        # Mock security test results
        security_issues_found = 2
        critical_vulnerabilities = 0

        if critical_vulnerabilities > 0:
            status = 'FAIL'
            details = f"Critical security vulnerabilities found: {critical_vulnerabilities}"
            grade = 'F'
            score = 0.0
        elif security_issues_found > 5:
            status = 'FAIL'
            details = f"Multiple security issues found: {security_issues_found}"
            grade = 'C'
            score = 60.0
        elif security_issues_found > 0:
            status = 'PASS'
            details = f"Minor security issues found: {security_issues_found}"
            grade = 'B'
            score = 80.0
        else:
            status = 'PASS'
            details = "No critical security issues found"
            grade = 'A'
            score = 95.0

        critical_issues = []
        if critical_vulnerabilities > 0:
            critical_issues.append(f"{critical_vulnerabilities} critical vulnerabilities")

        return TestSuiteResult(
            suite_name='Security Assessment',
            status=status,
            execution_time=2.0,
            details=details,
            grade=grade,
            score=score,
            critical_issues=critical_issues
        )

    def _grade_to_score(self, grade: str) -> float:
        """Convert letter grade to numeric score"""
        grade_scores = {
            'A': 95.0,
            'B+': 87.0,
            'B': 82.0,
            'C': 72.0,
            'F': 25.0,
            'NO_GRADE': 0.0
        }
        return grade_scores.get(grade, 0.0)

    async def _generate_master_report(self, test_results: List[TestSuiteResult],
                                    execution_time: float, verbose: bool) -> MasterTestReport:
        """Generate master test report"""
        # Calculate weighted overall score
        total_score = 0.0
        total_weight = 0.0
        critical_failures = 0
        failed_suites = 0

        for result in test_results:
            suite_config = next(
                (s for s in self.test_suites if s['name'] == result.suite_name),
                {'weight': 0.25, 'critical': True}
            )

            weight = suite_config['weight']
            total_score += result.score * weight
            total_weight += weight

            if suite_config['critical'] and result.status in ['FAIL', 'ERROR']:
                critical_failures += 1

            if result.status in ['FAIL', 'ERROR']:
                failed_suites += 1

        overall_score = total_score / total_weight if total_weight > 0 else 0.0

        # Determine overall grade
        if overall_score >= 90 and critical_failures == 0:
            overall_grade = 'A'
            overall_status = 'PASS'
        elif overall_score >= 80 and critical_failures == 0:
            overall_grade = 'B+'
            overall_status = 'PASS'
        elif overall_score >= 70 and critical_failures <= 1:
            overall_grade = 'B'
            overall_status = 'PASS'
        elif overall_score >= 60 and critical_failures <= 2:
            overall_grade = 'C'
            overall_status = 'FAIL'
        else:
            overall_grade = 'F'
            overall_status = 'FAIL'

        # Generate summary
        total_suites = len(test_results)
        passed_suites = len([r for r in test_results if r.status == 'PASS'])
        failed_suites = len([r for r in test_results if r.status in ['FAIL', 'ERROR']])
        skipped_suites = len([r for r in test_results if r.status == 'SKIPPED'])

        # Collect all critical issues
        all_critical_issues = []
        for result in test_results:
            if result.critical_issues:
                all_critical_issues.extend(result.critical_issues)

        # Production readiness assessment
        production_readiness = {
            'ready': overall_status == 'PASS' and critical_failures == 0,
            'critical_failures': critical_failures,
            'blockers': len(all_critical_issues),
            'confidence_level': min(overall_score, 100),
            'recommended_actions': []
        }

        if critical_failures > 0:
            production_readiness['recommended_actions'].append(
                "CRITICAL: Fix all critical test failures before deployment"
            )
        if overall_score < 80:
            production_readiness['recommended_actions'].append(
                "Address performance and quality issues before production"
            )
        if production_readiness['confidence_level'] < 70:
            production_readiness['recommended_actions'].append(
                "Increase test coverage and quality before production deployment"
            )

        # Generate executive summary
        executive_summary = self._generate_executive_summary(
            overall_grade, overall_score, failed_suites, critical_failures, execution_time
        )

        # Generate recommendations
        recommendations = self._generate_recommendations(test_results, overall_score, critical_failures)

        return MasterTestReport(
            timestamp=datetime.now(timezone.utc).isoformat(),
            overall_status=overall_status,
            overall_grade=overall_grade,
            overall_score=overall_score,
            execution_time=execution_time,
            test_suites=test_results,
            summary={
                'total_suites': total_suites,
                'passed': passed_suites,
                'failed': failed_suites,
                'skipped': skipped_suites,
                'success_rate': (passed_suites / total_suites * 100) if total_suites > 0 else 0,
                'critical_failures': critical_failures
            },
            recommendations=recommendations,
            production_readiness=production_readiness,
            executive_summary=executive_summary
        )

    def _generate_executive_summary(self, grade: str, score: float,
                                  failed_suites: int, critical_failures: int,
                                  execution_time: float) -> str:
        """Generate executive summary"""
        status_emoji = "‚úÖ" if grade in ['A', 'B+', 'B'] else "‚ö†Ô∏è" if grade == 'C' else "‚ùå"

        summary = f"""
{status_emoji} PsychSync AI Platform - Test Results Summary

Overall Grade: {grade} (Score: {score:.1f}%)
Test Execution Time: {execution_time:.1f} seconds
Failed Test Suites: {failed_suites} ({critical_failures} critical)

"""

        if grade in ['A', 'B+', 'B']:
            summary += """The platform demonstrates strong quality and is ready for production deployment.
All critical test suites have passed with acceptable performance metrics.
The application architecture is sound, functionality is working as expected,
and performance meets requirements.
"""
        elif grade == 'C':
            summary += """The platform shows some quality issues that should be addressed before production.
While basic functionality is working, there are areas that need improvement
to ensure reliability and performance in production environments.
"""
        else:
            summary += """The platform has significant quality issues that prevent production deployment.
Critical failures in multiple test suites indicate serious problems
that must be resolved before considering deployment to any environment.
"""

        return summary.strip()

    def _generate_recommendations(self, test_results: List[TestSuiteResult],
                                overall_score: float, critical_failures: int) -> List[str]:
        """Generate comprehensive recommendations"""
        recommendations = []

        # Critical recommendations
        if critical_failures > 0:
            recommendations.append(
                "üö® CRITICAL: All critical test failures must be resolved before production deployment"
            )

        # Overall quality recommendations
        if overall_score < 70:
            recommendations.append(
                "Significant quality improvements needed - comprehensive code review and refactoring required"
            )
        elif overall_score < 85:
            recommendations.append(
                "Quality improvements recommended - address failing tests and optimize performance"
            )

        # Suite-specific recommendations
        for result in test_results:
            if result.status in ['FAIL', 'ERROR']:
                recommendations.append(
                    f"üîß Fix {result.suite_name.lower()} issues: {result.details}"
                )

        # General best practices
        recommendations.extend([
            "üìä Implement automated testing in CI/CD pipeline",
            "üîç Set up continuous monitoring and alerting",
            "üìà Establish performance baselines and monitoring",
            "üõ°Ô∏è Regular security assessments and vulnerability scanning",
            "üìö Maintain comprehensive test documentation"
        ])

        return recommendations

    async def _save_master_report(self, report: MasterTestReport):
        """Save master report to files"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # Save detailed JSON report
        json_path = self.test_reports_dir / f"master_test_report_{timestamp}.json"
        with open(json_path, 'w') as f:
            json.dump(asdict(report), f, indent=2)

        # Save executive summary (markdown)
        md_path = self.test_reports_dir / f"executive_summary_{timestamp}.md"
        with open(md_path, 'w') as f:
            f.write(f"# PsychSync AI - Test Executive Summary\n\n")
            f.write(f"**Generated:** {report.timestamp}\n\n")
            f.write(f"## Overall Result: {report.overall_grade} ({report.overall_score:.1f}%)\n\n")
            f.write(report.executive_summary)
            f.write("\n\n## Production Readiness\n\n")
            readiness = report.production_readiness
            status = "‚úÖ READY" if readiness['ready'] else "‚ùå NOT READY"
            f.write(f"**Status:** {status}\n")
            f.write(f"**Confidence Level:** {readiness['confidence_level']:.1f}%\n")
            f.write(f"**Critical Failures:** {readiness['critical_failures']}\n")
            f.write(f"**Blockers:** {readiness['blockers']}\n\n")

            if readiness['recommended_actions']:
                f.write("### Recommended Actions\n\n")
                for action in readiness['recommended_actions']:
                    f.write(f"- {action}\n")

        logger.info(f"Master reports saved to {json_path} and {md_path}")

    def _print_executive_summary(self, report: MasterTestReport):
        """Print executive summary to console"""
        print(f"\n{'='*60}")
        print(f"üìä PSYCHSYNC AI - MASTER TEST REPORT")
        print(f"{'='*60}")

        status_emoji = "‚úÖ" if report.overall_grade in ['A', 'B+', 'B'] else "‚ö†Ô∏è" if report.overall_grade == 'C' else "‚ùå"
        print(f"\n{status_emoji} OVERALL RESULT: {report.overall_grade} ({report.overall_score:.1f}%)")
        print(f"üïê Execution Time: {report.execution_time:.1f} seconds")
        print(f"üìà Success Rate: {report.summary['success_rate']:.1f}%")

        print(f"\nüìã Test Suite Results:")
        for suite in report.test_suites:
            status_icon = "‚úÖ" if suite.status == 'PASS' else "‚ùå" if suite.status in ['FAIL', 'ERROR'] else "‚è≠Ô∏è"
            grade_info = f" (Grade: {suite.grade})" if suite.grade else ""
            print(f"   {status_icon} {suite.suite_name}{grade_info}")

        print(f"\nüè≠ Production Readiness:")
        readiness = report.production_readiness
        status = "‚úÖ READY" if readiness['ready'] else "‚ùå NOT READY"
        print(f"   Status: {status}")
        print(f"   Confidence: {readiness['confidence_level']:.1f}%")
        print(f"   Critical Issues: {readiness['critical_failures']}")

        if readiness['recommended_actions']:
            print(f"\n‚ö†Ô∏è  Recommended Actions:")
            for action in readiness['recommended_actions']:
                print(f"   ‚Ä¢ {action}")

        print(f"\nüìÑ Full reports saved to: {self.test_reports_dir}")
        print(f"{'='*60}")

# CLI interface
def create_cli():
    """Create command line interface"""
    parser = argparse.ArgumentParser(
        description="PsychSync AI - Master Test Orchestrator",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python devops_master_test_orchestrator.py                    # Run all tests
  python devops_master_test_orchestrator.py --skip-non-critical  # Skip non-critical tests
  python devops_master_test_orchestrator.py --parallel          # Run tests in parallel
  python devops_master_test_orchestrator.py --architecture-only # Run only architecture tests
        """
    )

    parser.add_argument(
        '--skip-non-critical',
        action='store_true',
        help='Skip non-critical test suites'
    )

    parser.add_argument(
        '--parallel',
        action='store_true',
        help='Run test suites in parallel'
    )

    parser.add_argument(
        '--quiet',
        action='store_true',
        help='Reduce output verbosity'
    )

    parser.add_argument(
        '--architecture-only',
        action='store_true',
        help='Run only architecture validation tests'
    )

    parser.add_argument(
        '--functional-only',
        action='store_true',
        help='Run only functional testing'
    )

    parser.add_argument(
        '--performance-only',
        action='store_true',
        help='Run only performance testing'
    )

    parser.add_argument(
        '--security-only',
        action='store_true',
        help='Run only security assessment'
    )

    parser.add_argument(
        '--ci-mode',
        action='store_true',
        help='Run in CI mode with minimal output and exit codes'
    )

    return parser

async def run_single_test_suite(suite_name: str, orchestrator: MasterTestOrchestrator,
                               verbose: bool = True) -> MasterTestReport:
    """Run a single test suite"""
    suite_map = {
        'architecture': orchestrator._run_architecture_tests,
        'functional': orchestrator._run_functional_tests,
        'performance': orchestrator._run_performance_tests,
        'security': orchestrator._run_security_tests
    }

    if suite_name not in suite_map:
        raise ValueError(f"Unknown test suite: {suite_name}")

    print(f"üß™ Running {suite_name} tests...")
    start_time = time.time()

    try:
        result = await suite_map[suite_name]()
        execution_time = time.time() - start_time
        result.execution_time = execution_time

        # Generate minimal master report
        master_report = MasterTestReport(
            timestamp=datetime.now(timezone.utc).isoformat(),
            overall_status=result.status,
            overall_grade=result.grade or 'NO_GRADE',
            overall_score=result.score,
            execution_time=execution_time,
            test_suites=[result],
            summary={
                'total_suites': 1,
                'passed': 1 if result.status == 'PASS' else 0,
                'failed': 1 if result.status in ['FAIL', 'ERROR'] else 0,
                'skipped': 1 if result.status == 'SKIPPED' else 0,
                'success_rate': 100 if result.status == 'PASS' else 0,
                'critical_failures': len(result.critical_issues)
            },
            recommendations=[],
            production_readiness={
                'ready': result.status == 'PASS' and not result.critical_issues,
                'critical_failures': len(result.critical_issues),
                'blockers': len(result.critical_issues),
                'confidence_level': result.score,
                'recommended_actions': result.critical_issues
            },
            executive_summary=f"Single test suite execution: {result.details}"
        )

        await orchestrator._save_master_report(master_report)

        if verbose:
            print(f"\n{'='*40}")
            print(f"üìä {suite_name.title()} Test Results")
            print(f"{'='*40}")
            print(f"Status: {result.status}")
            print(f"Details: {result.details}")
            if result.grade:
                print(f"Grade: {result.grade}")
            if result.critical_issues:
                print("Critical Issues:")
                for issue in result.critical_issues:
                    print(f"  ‚Ä¢ {issue}")

        return master_report

    except Exception as e:
        logger.error(f"Test suite {suite_name} failed: {str(e)}")
        if verbose:
            print(f"‚ùå Test suite failed: {str(e)}")
        raise

async def main():
    """Main execution function"""
    parser = create_cli()
    args = parser.parse_args()

    orchestrator = MasterTestOrchestrator()

    try:
        # Handle single suite execution
        if args.architecture_only:
            report = await run_single_test_suite('architecture', orchestrator, not args.quiet)
        elif args.functional_only:
            report = await run_single_test_suite('functional', orchestrator, not args.quiet)
        elif args.performance_only:
            report = await run_single_test_suite('performance', orchestrator, not args.quiet)
        elif args.security_only:
            report = await run_single_test_suite('security', orchestrator, not args.quiet)
        else:
            # Run all test suites
            report = await orchestrator.run_all_tests(
                skip_non_critical=args.skip_non_critical,
                parallel=args.parallel,
                verbose=not args.quiet
            )

        # CI mode handling
        if args.ci_mode:
            # Exit with appropriate code for CI systems
            if report.overall_status == 'PASS':
                print("‚úÖ CI CHECK: PASSED")
                sys.exit(0)
            else:
                print("‚ùå CI CHECK: FAILED")
                sys.exit(1)

        return report

    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Testing interrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.error(f"Test execution failed: {str(e)}")
        if not args.quiet:
            print(f"‚ùå Test execution failed: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())